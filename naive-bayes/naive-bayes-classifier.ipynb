{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "from os import listdir\n",
                "import re\n",
                "from typing import List, Counter\n",
                "import numpy as np\n",
                "import collections\n",
                "import operator\n",
                "from sklearn.model_selection import train_test_split"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "def strip(word: str, chars_to_strip: List[str]) -> str:\n",
                "  for char in chars_to_strip:\n",
                "    word = word.strip(char)\n",
                "      \n",
                "  return word\n",
                "\n",
                "def format_word(word: str) -> str:\n",
                "  return strip(word, ['\"', \"'\", \"(\", \")\"])\n",
                "\n",
                "def tokenize(doc: str) -> List[str]:\n",
                "    doc=doc.lower()\n",
                "    doc=re.sub('\\n',' ',doc)\n",
                "    doc=re.sub('[,.+=]',' ',doc)\n",
                "    doc=re.sub('[0-9]','',doc)\n",
                "    doc=re.sub('\\s+',' ',doc)\n",
                "    doc=doc.strip()\n",
                "    return [w for w in [format_word(word) for word in doc.split()] if len(w) > 0]\n",
                "\n",
                "def read_docs(category: str, target: str):\n",
                "  categories = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
                "\n",
                "  if category not in categories:\n",
                "    raise NameError(f'Category {category} not found')\n",
                "\n",
                "  directory = f'./bbc/{category}/'\n",
                "  filenames = [f'{directory}{f}' for f in listdir(directory) if f.endswith('.txt')]\n",
                "\n",
                "  for file in filenames:\n",
                "    with open(file, encoding='latin-1') as f:\n",
                "      yield Document(f.read(), target)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "class Document:\n",
                "  def __init__(self, text, target):\n",
                "    self.text = text\n",
                "    self.target = target\n",
                "    self.words = tokenize(self.text)\n",
                "\n",
                "  def get_vocabulary(self):\n",
                "    return collections.Counter(self.words)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "class NaiveBayes:\n",
                "  def __init__(self) -> None:\n",
                "      self.log_prior = {}\n",
                "      self.log_likelihood = {}\n",
                "      self.targets = []\n",
                "      self.vocabulary = {}\n",
                "\n",
                "  def train(self, documents: List[Document]):\n",
                "    docs_count = len(documents)\n",
                "    self.targets = self.get_targets(documents)\n",
                "    self.vocabulary = collections.Counter()\n",
                "\n",
                "    for doc in documents: \n",
                "      self.vocabulary += doc.get_vocabulary()\n",
                "\n",
                "    for target in self.targets:\n",
                "      target_docs = self.get_target_documents(documents, target)\n",
                "      target_docs_count = len(target_docs)\n",
                "\n",
                "      self.log_prior[target] = np.log(target_docs_count/docs_count)\n",
                "      \n",
                "      target_vocabulary = self.get_docs_vocabulary(target_docs)\n",
                "      target_words_count = sum(target_vocabulary.values())\n",
                "\n",
                "      for word in self.vocabulary.keys():\n",
                "        word_count = target_vocabulary[word]\n",
                "        self.log_likelihood[f'{target}-{word}'] = np.log( (word_count + 1) / (target_words_count + 1) )\n",
                "\n",
                "    return self\n",
                "\n",
                "  def test(self, document: Document, stop_words_count=50) -> str:\n",
                "    target_sum  = {}\n",
                "    stop_words = [word for (word, _) in self.vocabulary.most_common(stop_words_count)]\n",
                "\n",
                "    for target in self.targets:\n",
                "      target_sum[target] = self.log_prior[target]\n",
                "\n",
                "      for word in document.words:\n",
                "        if word not in stop_words:\n",
                "          try:\n",
                "            target_sum[target] += self.log_likelihood[f'{target}-{word}']\n",
                "          except KeyError:\n",
                "            pass\n",
                "\n",
                "    (max_index, _) = max(enumerate(list(target_sum.values())), key=operator.itemgetter(1))\n",
                "    return list(target_sum)[max_index]\n",
                "\n",
                "  def score(self, documents: List[Document]) -> float:\n",
                "    success_count = 0\n",
                "    documents_count = len(documents)\n",
                "\n",
                "    for document in documents:\n",
                "      prediction = self.test(document)\n",
                "      if prediction == document.target:\n",
                "        success_count += 1\n",
                "\n",
                "    return {\n",
                "      'accuracy': success_count / documents_count,\n",
                "    }\n",
                "\n",
                "  def get_targets(self, documents: List[Document]) -> List[str]:\n",
                "    return list(collections.Counter([doc.target for doc in documents]).keys())\n",
                "\n",
                "  def get_target_documents(self, documents: List[Document], target: str) -> List[Document]:\n",
                "    target_docs = [doc for doc in documents if doc.target == target]\n",
                "    return target_docs\n",
                "\n",
                "  def get_docs_vocabulary(self, documents: List[Document]) -> Counter:\n",
                "    vocabulary = collections.Counter()\n",
                "    for doc in documents:\n",
                "      vocabulary += doc.get_vocabulary()\n",
                "\n",
                "    return vocabulary\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "dataset = [doc for doc in read_docs('tech', 'true')] + [doc for doc in read_docs('business', 'false')] + [doc for doc in read_docs('politics', 'false')] + [doc for doc in read_docs('entertainment', 'false')]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(dataset, dataset, test_size=0.3, random_state=42)\n",
                "classifier = NaiveBayes().train(X_train)\n",
                "result = classifier.score(X_test)\n",
                "print(result)\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{'accuracy': 0.9669902912621359}\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.0",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.0 64-bit"
        },
        "interpreter": {
            "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}